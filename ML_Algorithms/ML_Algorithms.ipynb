{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c402691-c1e6-48cc-9497-c7b155e3784d",
   "metadata": {},
   "source": [
    "# Exploring various ML algorithms using Numpy and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f266f-7182-4c5b-a078-3a0fdca3382d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69e88b15-47a1-4176-9f54-249deeab8dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 44.0000\n",
      "Epoch 10: Loss = 34.6899\n",
      "Epoch 20: Loss = 27.3519\n",
      "Epoch 30: Loss = 21.5682\n",
      "Epoch 40: Loss = 17.0095\n",
      "Epoch 50: Loss = 13.4165\n",
      "Epoch 60: Loss = 10.5844\n",
      "Epoch 70: Loss = 8.3523\n",
      "Epoch 80: Loss = 6.5929\n",
      "Epoch 90: Loss = 5.2062\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "class LR: \n",
    "    def __init__(self, lr=0.001, iters= 100): \n",
    "        self.lr= lr\n",
    "        self.weights= None \n",
    "        self.bias= None \n",
    "        self.iter= iters\n",
    "\n",
    "    def fit(self, X_train, y_train): \n",
    "        n_samples, n_features= X_train.shape\n",
    "        self.weights= np.zeros(n_features)\n",
    "        self.bias= 0\n",
    "        for _ in range(self.iter): \n",
    "            y_pred= np.dot(X_train, self.weights) + self.bias\n",
    "            loss = ((y_pred - y_train)**2).mean()\n",
    "            dw= (1/n_samples) * np.dot(X_train.T , (y_pred - y_train))\n",
    "            db= (1/n_samples) * np.sum(y_pred - y_train)\n",
    "\n",
    "            self.weights -= dw * self.lr \n",
    "            self.bias -= db * self.lr \n",
    "\n",
    "            if _%10 ==0 : \n",
    "                print(f\"Epoch {_}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred= np.dot(X,  self.weights) + self.bias \n",
    "        return y_pred\n",
    "LR= LR()\n",
    "x = np.array([1, 2, 3, 4, 5], dtype=np.float32).reshape(-1, 1)\n",
    "y = np.array([2, 4, 6, 8, 10], dtype=np.float32)\n",
    "LR.fit(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4361f35-cb9e-41ec-bee3-6e747fdb3beb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Regression with custom loss function (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb802e2c-9aa8-4f62-ad8a-831282ea744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.0246\n",
      "Epoch 10: Loss = 0.0246\n",
      "Epoch 20: Loss = 0.0246\n",
      "Epoch 30: Loss = 0.0246\n",
      "Epoch 40: Loss = 0.0246\n",
      "Epoch 50: Loss = 0.0246\n",
      "Epoch 60: Loss = 0.0246\n",
      "Epoch 70: Loss = 0.0246\n",
      "Epoch 80: Loss = 0.0246\n",
      "Epoch 90: Loss = 0.0246\n"
     ]
    }
   ],
   "source": [
    "# changing loss function to MAE \n",
    "import numpy as np \n",
    "\n",
    "class LR: \n",
    "    def __init__(self, lr=0.001, iters= 100): \n",
    "        self.lr= lr\n",
    "        self.weights= None \n",
    "        self.bias= None \n",
    "        self.iter= iters\n",
    "    def fit(self, X_train, y_train): \n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "    \n",
    "        for _ in range(self.iter): \n",
    "            y_pred = np.dot(X_train, self.weights) + self.bias\n",
    "            error = y_pred - y_train\n",
    "            sign = np.sign(error)\n",
    "    \n",
    "            dw = (1/n_samples) * np.dot(X_train.T, sign)\n",
    "            db = (1/n_samples) * np.sum(sign)\n",
    "    \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            if _%10 ==0 : \n",
    "                print(f\"Epoch {_}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred= np.dot(X,  self.weights) + self.bias \n",
    "        return y_pred\n",
    "LR= LR()\n",
    "x = np.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)\n",
    "y = np.array([2, 4, 6], dtype=np.float32)\n",
    "LR.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b0b4d-898a-4205-b1d0-7f0b016b0284",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Regression using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5043fb-53c9-4933-ac29-1d4f95377172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 90.7753, w = -0.36, b = 0.61\n",
      "Epoch 10: Loss = 0.6434, w = 1.55, b = 1.09\n",
      "Epoch 20: Loss = 0.2210, w = 1.69, b = 1.09\n",
      "Epoch 30: Loss = 0.2048, w = 1.71, b = 1.06\n",
      "Epoch 40: Loss = 0.1914, w = 1.72, b = 1.02\n",
      "Epoch 50: Loss = 0.1788, w = 1.73, b = 0.99\n",
      "Epoch 60: Loss = 0.1671, w = 1.74, b = 0.96\n",
      "Epoch 70: Loss = 0.1562, w = 1.74, b = 0.92\n",
      "Epoch 80: Loss = 0.1460, w = 1.75, b = 0.89\n",
      "Epoch 90: Loss = 0.1364, w = 1.76, b = 0.86\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8, 10], dtype=torch.float32)\n",
    "\n",
    "# Parameters\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        # Clears the accumulated gradients after each update.\n",
    "        # ⚠️ Without this, gradients would accumulate across epochs (default PyTorch behavior).\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, w = {w.item():.2f}, b = {b.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001d929-5baa-4e49-b2b3-9841bb337bcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Logistic Regression using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01d321ad-d0f8-4928-b18a-77e4bb7ba38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is : 0.6931\n",
      "loss at epoch 10 is : 0.5445\n",
      "loss at epoch 20 is : 0.5394\n",
      "loss at epoch 30 is : 0.5346\n",
      "loss at epoch 40 is : 0.5298\n",
      "loss at epoch 50 is : 0.5251\n",
      "loss at epoch 60 is : 0.5205\n",
      "loss at epoch 70 is : 0.5160\n",
      "loss at epoch 80 is : 0.5115\n",
      "loss at epoch 90 is : 0.5070\n",
      "loss at epoch 100 is : 0.5027\n",
      "loss at epoch 110 is : 0.4983\n",
      "loss at epoch 120 is : 0.4941\n",
      "loss at epoch 130 is : 0.4899\n",
      "loss at epoch 140 is : 0.4857\n",
      "loss at epoch 150 is : 0.4816\n",
      "loss at epoch 160 is : 0.4776\n",
      "loss at epoch 170 is : 0.4736\n",
      "loss at epoch 180 is : 0.4696\n",
      "loss at epoch 190 is : 0.4658\n",
      "loss at epoch 200 is : 0.4619\n",
      "loss at epoch 210 is : 0.4581\n",
      "loss at epoch 220 is : 0.4544\n",
      "loss at epoch 230 is : 0.4507\n",
      "loss at epoch 240 is : 0.4471\n",
      "loss at epoch 250 is : 0.4435\n",
      "loss at epoch 260 is : 0.4399\n",
      "loss at epoch 270 is : 0.4364\n",
      "loss at epoch 280 is : 0.4330\n",
      "loss at epoch 290 is : 0.4296\n",
      "loss at epoch 300 is : 0.4262\n",
      "loss at epoch 310 is : 0.4229\n",
      "loss at epoch 320 is : 0.4196\n",
      "loss at epoch 330 is : 0.4164\n",
      "loss at epoch 340 is : 0.4132\n",
      "loss at epoch 350 is : 0.4100\n",
      "loss at epoch 360 is : 0.4069\n",
      "loss at epoch 370 is : 0.4038\n",
      "loss at epoch 380 is : 0.4008\n",
      "loss at epoch 390 is : 0.3978\n",
      "loss at epoch 400 is : 0.3948\n",
      "loss at epoch 410 is : 0.3919\n",
      "loss at epoch 420 is : 0.3890\n",
      "loss at epoch 430 is : 0.3862\n",
      "loss at epoch 440 is : 0.3834\n",
      "loss at epoch 450 is : 0.3806\n",
      "loss at epoch 460 is : 0.3778\n",
      "loss at epoch 470 is : 0.3751\n",
      "loss at epoch 480 is : 0.3725\n",
      "loss at epoch 490 is : 0.3698\n",
      "y_pred : [1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.75, 0.499999999975, 0.9999999999)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogReg: \n",
    "    def __init__(self, iter= 500, lr= 0.01): \n",
    "        self.iter = iter\n",
    "        self.lr = lr \n",
    "        \n",
    "    def sigmoid(self, y): \n",
    "        return (1/(1+np.exp(-1*y)))\n",
    "                \n",
    "    def compute_loss(self, y_pred, y): \n",
    "        # handle 0 case in log \n",
    "        epsilon = np.exp(-15)\n",
    "        y_pred= np.clip(y_pred, epsilon, 1-epsilon)       \n",
    "        loss= -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "        return loss \n",
    "        \n",
    "    def fit(self, X, y): \n",
    "        X= np.array(X)\n",
    "        y= np.array(y)\n",
    "        n_samples , n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0 \n",
    "        for _ in range(self.iter): \n",
    "            # forward pass \n",
    "            y_pred_linear = np.dot(X, self.weights) + self.bias \n",
    "            y_pred= self.sigmoid(y_pred_linear)\n",
    "            # compute loss \n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            # backward pass \n",
    "            dw= (1/n_samples)*np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples)*np.sum(y_pred - y)\n",
    "            # update weights \n",
    "            self.weights -= self.lr*(dw)\n",
    "            self.bias -= self.lr*(db)\n",
    "\n",
    "            # track losses \n",
    "            if _%10==0: \n",
    "                print(f\"loss at epoch {_} is : {loss:.4f}\")\n",
    "\n",
    "    def predict(self , X): \n",
    "        y=  self.sigmoid(np.dot(X, self.weights)+ self.bias)\n",
    "        # return [1 if i>0.5 else 0 for i in y]\n",
    "        return (y>0.5).astype(int)\n",
    "        \n",
    "    def evaluate(self, y_pred, y): \n",
    "        y_pred= np.array(y_pred)\n",
    "        y= np.array(y)\n",
    "        # accuracy = np.sum(y_pred==y)/len(y)\n",
    "        TP = np.sum((y_pred==1) & (y==1))\n",
    "        FN = np.sum((y_pred==0) & (y==1))\n",
    "        FP = np.sum((y_pred==1)& (y==0))\n",
    "        TN = np.sum((y_pred==0)&(y==0))\n",
    "        recall = TP/(TP+FN+1e-10)\n",
    "        precision = TP/(TP+FP+1e-10)\n",
    "        accuracy = (TP+TN)/len(y)\n",
    "        return accuracy, recall, precision\n",
    "\n",
    "X= [[2, 2], [4, 4], [10, 10], [15, 15]]\n",
    "y= [1, 1, 0, 0] \n",
    "LR= LogReg()\n",
    "LR.fit(X, y)\n",
    "y_pred= LR.predict(X)\n",
    "print(f\"y_pred : {y_pred}\")\n",
    "LR.evaluate(y_pred, y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11faa300-e602-47b2-a60b-7de64bb5b331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
