{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c402691-c1e6-48cc-9497-c7b155e3784d",
   "metadata": {},
   "source": [
    "# Exploring various ML algorithms using Numpy and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f266f-7182-4c5b-a078-3a0fdca3382d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69e88b15-47a1-4176-9f54-249deeab8dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 44.0000\n",
      "Epoch 10: Loss = 34.6899\n",
      "Epoch 20: Loss = 27.3519\n",
      "Epoch 30: Loss = 21.5682\n",
      "Epoch 40: Loss = 17.0095\n",
      "Epoch 50: Loss = 13.4165\n",
      "Epoch 60: Loss = 10.5844\n",
      "Epoch 70: Loss = 8.3523\n",
      "Epoch 80: Loss = 6.5929\n",
      "Epoch 90: Loss = 5.2062\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "class LR: \n",
    "    def __init__(self, lr=0.001, iters= 100): \n",
    "        self.lr= lr\n",
    "        self.weights= None \n",
    "        self.bias= None \n",
    "        self.iter= iters\n",
    "\n",
    "    def fit(self, X_train, y_train): \n",
    "        n_samples, n_features= X_train.shape\n",
    "        self.weights= np.zeros(n_features)\n",
    "        self.bias= 0\n",
    "        for _ in range(self.iter): \n",
    "            y_pred= np.dot(X_train, self.weights) + self.bias\n",
    "            loss = ((y_pred - y_train)**2).mean()\n",
    "            dw= (1/n_samples) * np.dot(X_train.T , (y_pred - y_train))\n",
    "            db= (1/n_samples) * np.sum(y_pred - y_train)\n",
    "\n",
    "            self.weights -= dw * self.lr \n",
    "            self.bias -= db * self.lr \n",
    "\n",
    "            if _%10 ==0 : \n",
    "                print(f\"Epoch {_}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred= np.dot(X,  self.weights) + self.bias \n",
    "        return y_pred\n",
    "LR= LR()\n",
    "x = np.array([1, 2, 3, 4, 5], dtype=np.float32).reshape(-1, 1)\n",
    "y = np.array([2, 4, 6, 8, 10], dtype=np.float32)\n",
    "LR.fit(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4361f35-cb9e-41ec-bee3-6e747fdb3beb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Regression with custom loss function (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb802e2c-9aa8-4f62-ad8a-831282ea744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.0246\n",
      "Epoch 10: Loss = 0.0246\n",
      "Epoch 20: Loss = 0.0246\n",
      "Epoch 30: Loss = 0.0246\n",
      "Epoch 40: Loss = 0.0246\n",
      "Epoch 50: Loss = 0.0246\n",
      "Epoch 60: Loss = 0.0246\n",
      "Epoch 70: Loss = 0.0246\n",
      "Epoch 80: Loss = 0.0246\n",
      "Epoch 90: Loss = 0.0246\n"
     ]
    }
   ],
   "source": [
    "# changing loss function to MAE \n",
    "import numpy as np \n",
    "\n",
    "class LR: \n",
    "    def __init__(self, lr=0.001, iters= 100): \n",
    "        self.lr= lr\n",
    "        self.weights= None \n",
    "        self.bias= None \n",
    "        self.iter= iters\n",
    "    def fit(self, X_train, y_train): \n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "    \n",
    "        for _ in range(self.iter): \n",
    "            y_pred = np.dot(X_train, self.weights) + self.bias\n",
    "            error = y_pred - y_train\n",
    "            sign = np.sign(error)\n",
    "    \n",
    "            dw = (1/n_samples) * np.dot(X_train.T, sign)\n",
    "            db = (1/n_samples) * np.sum(sign)\n",
    "    \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            if _%10 ==0 : \n",
    "                print(f\"Epoch {_}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred= np.dot(X,  self.weights) + self.bias \n",
    "        return y_pred\n",
    "LR= LR()\n",
    "x = np.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)\n",
    "y = np.array([2, 4, 6], dtype=np.float32)\n",
    "LR.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b0b4d-898a-4205-b1d0-7f0b016b0284",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Regression using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5043fb-53c9-4933-ac29-1d4f95377172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 90.7753, w = -0.36, b = 0.61\n",
      "Epoch 10: Loss = 0.6434, w = 1.55, b = 1.09\n",
      "Epoch 20: Loss = 0.2210, w = 1.69, b = 1.09\n",
      "Epoch 30: Loss = 0.2048, w = 1.71, b = 1.06\n",
      "Epoch 40: Loss = 0.1914, w = 1.72, b = 1.02\n",
      "Epoch 50: Loss = 0.1788, w = 1.73, b = 0.99\n",
      "Epoch 60: Loss = 0.1671, w = 1.74, b = 0.96\n",
      "Epoch 70: Loss = 0.1562, w = 1.74, b = 0.92\n",
      "Epoch 80: Loss = 0.1460, w = 1.75, b = 0.89\n",
      "Epoch 90: Loss = 0.1364, w = 1.76, b = 0.86\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8, 10], dtype=torch.float32)\n",
    "\n",
    "# Parameters\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        # Clears the accumulated gradients after each update.\n",
    "        # âš ï¸ Without this, gradients would accumulate across epochs (default PyTorch behavior).\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, w = {w.item():.2f}, b = {b.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001d929-5baa-4e49-b2b3-9841bb337bcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Logistic Regression using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01d321ad-d0f8-4928-b18a-77e4bb7ba38e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is : 0.6931\n",
      "loss at epoch 10 is : 0.5445\n",
      "loss at epoch 20 is : 0.5394\n",
      "loss at epoch 30 is : 0.5346\n",
      "loss at epoch 40 is : 0.5298\n",
      "loss at epoch 50 is : 0.5251\n",
      "loss at epoch 60 is : 0.5205\n",
      "loss at epoch 70 is : 0.5160\n",
      "loss at epoch 80 is : 0.5115\n",
      "loss at epoch 90 is : 0.5070\n",
      "loss at epoch 100 is : 0.5027\n",
      "loss at epoch 110 is : 0.4983\n",
      "loss at epoch 120 is : 0.4941\n",
      "loss at epoch 130 is : 0.4899\n",
      "loss at epoch 140 is : 0.4857\n",
      "loss at epoch 150 is : 0.4816\n",
      "loss at epoch 160 is : 0.4776\n",
      "loss at epoch 170 is : 0.4736\n",
      "loss at epoch 180 is : 0.4696\n",
      "loss at epoch 190 is : 0.4658\n",
      "loss at epoch 200 is : 0.4619\n",
      "loss at epoch 210 is : 0.4581\n",
      "loss at epoch 220 is : 0.4544\n",
      "loss at epoch 230 is : 0.4507\n",
      "loss at epoch 240 is : 0.4471\n",
      "loss at epoch 250 is : 0.4435\n",
      "loss at epoch 260 is : 0.4399\n",
      "loss at epoch 270 is : 0.4364\n",
      "loss at epoch 280 is : 0.4330\n",
      "loss at epoch 290 is : 0.4296\n",
      "loss at epoch 300 is : 0.4262\n",
      "loss at epoch 310 is : 0.4229\n",
      "loss at epoch 320 is : 0.4196\n",
      "loss at epoch 330 is : 0.4164\n",
      "loss at epoch 340 is : 0.4132\n",
      "loss at epoch 350 is : 0.4100\n",
      "loss at epoch 360 is : 0.4069\n",
      "loss at epoch 370 is : 0.4038\n",
      "loss at epoch 380 is : 0.4008\n",
      "loss at epoch 390 is : 0.3978\n",
      "loss at epoch 400 is : 0.3948\n",
      "loss at epoch 410 is : 0.3919\n",
      "loss at epoch 420 is : 0.3890\n",
      "loss at epoch 430 is : 0.3862\n",
      "loss at epoch 440 is : 0.3834\n",
      "loss at epoch 450 is : 0.3806\n",
      "loss at epoch 460 is : 0.3778\n",
      "loss at epoch 470 is : 0.3751\n",
      "loss at epoch 480 is : 0.3725\n",
      "loss at epoch 490 is : 0.3698\n",
      "y_pred : [1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.75, 0.499999999975, 0.9999999999)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogReg: \n",
    "    def __init__(self, iter= 500, lr= 0.01): \n",
    "        self.iter = iter\n",
    "        self.lr = lr \n",
    "        \n",
    "    def sigmoid(self, y): \n",
    "        return (1/(1+np.exp(-1*y)))\n",
    "                \n",
    "    def compute_loss(self, y_pred, y): \n",
    "        # handle 0 case in log \n",
    "        epsilon = np.exp(-15)\n",
    "        y_pred= np.clip(y_pred, epsilon, 1-epsilon)       \n",
    "        loss= -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "        return loss \n",
    "        \n",
    "    def fit(self, X, y): \n",
    "        X= np.array(X)\n",
    "        y= np.array(y)\n",
    "        n_samples , n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0 \n",
    "        for _ in range(self.iter): \n",
    "            # forward pass \n",
    "            y_pred_linear = np.dot(X, self.weights) + self.bias \n",
    "            y_pred= self.sigmoid(y_pred_linear)\n",
    "            # compute loss \n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            # backward pass \n",
    "            dw= (1/n_samples)*np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples)*np.sum(y_pred - y)\n",
    "            # update weights \n",
    "            self.weights -= self.lr*(dw)\n",
    "            self.bias -= self.lr*(db)\n",
    "\n",
    "            # track losses \n",
    "            if _%10==0: \n",
    "                print(f\"loss at epoch {_} is : {loss:.4f}\")\n",
    "\n",
    "    def predict(self , X): \n",
    "        y=  self.sigmoid(np.dot(X, self.weights)+ self.bias)\n",
    "        # return [1 if i>0.5 else 0 for i in y]\n",
    "        return (y>0.5).astype(int)\n",
    "        \n",
    "    def evaluate(self, y_pred, y): \n",
    "        y_pred= np.array(y_pred)\n",
    "        y= np.array(y)\n",
    "        # accuracy = np.sum(y_pred==y)/len(y)\n",
    "        TP = np.sum((y_pred==1) & (y==1))\n",
    "        FN = np.sum((y_pred==0) & (y==1))\n",
    "        FP = np.sum((y_pred==1)& (y==0))\n",
    "        TN = np.sum((y_pred==0)&(y==0))\n",
    "        recall = TP/(TP+FN+1e-10)\n",
    "        precision = TP/(TP+FP+1e-10)\n",
    "        accuracy = (TP+TN)/len(y)\n",
    "        return accuracy, recall, precision\n",
    "\n",
    "X= [[2, 2], [4, 4], [10, 10], [15, 15]]\n",
    "y= [1, 1, 0, 0] \n",
    "LR= LogReg()\n",
    "LR.fit(X, y)\n",
    "y_pred= LR.predict(X)\n",
    "print(f\"y_pred : {y_pred}\")\n",
    "LR.evaluate(y_pred, y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d10d6-c8e3-43fb-8978-b1e5317a6900",
   "metadata": {},
   "source": [
    "### Logistic Regression using torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dbe973-fc35-4d81-b7c9-f261d04521d0",
   "metadata": {},
   "source": [
    "### KNN with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1631e3e4-ef2a-42d4-8577-cac8f11ff4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bear']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "class KNN: \n",
    "    def __init__(self, K): \n",
    "        self.K= K\n",
    "\n",
    "    def fit(self, X, y, classification= True): \n",
    "        self.X= np.array(X)\n",
    "        self.y = np.array(y) \n",
    "        self.classification = classification \n",
    "\n",
    "    def euclidean_distance(self, i, x): \n",
    "        return np.sqrt(np.sum((i-x)**2))\n",
    "     \n",
    "    def _predict(self, x): \n",
    "        x= np.array(x)\n",
    "        distances= [ self.euclidean_distance(i, x) for i in self.X]\n",
    "        k_top = np.argsort(distances)[: self.K]\n",
    "        k_nearest_labels = [self.y[i] for i in k_top]\n",
    "        # take max of k nearest points \n",
    "        if self.classification : \n",
    "            return Counter(k_nearest_labels).most_common()[0][0]\n",
    "        else : \n",
    "            return np.mean(k_nearest_labels)\n",
    "\n",
    "    def predict(self, x): \n",
    "        return [ self._predict(i) for i in x ]\n",
    "             \n",
    "knn= KNN(K= 2)\n",
    "X = [[1], [2],[3], [4], [5], [2], [1]]\n",
    "y= ['dog', 'cat', 'dog', 'bear', 'cat', 'dog', 'cat']\n",
    "knn.fit(X, y, True)\n",
    "knn.predict([[4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911ea5b-f444-411f-8132-f81e89ef6cf4",
   "metadata": {},
   "source": [
    "### KMeans using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90836057-45b7-4426-adb7-3dd89d8dc5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 5])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1, 2, 3, 4, 5, 8, 9, 2, 3, 6, 5, 3, 2]\n",
    "np.random.choice(len(X), size= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "16ee555e-ffaa-416f-b777-c63549c84282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k, max_iterations=100):\n",
    "        self.k = k\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Initialize centroids randomly\n",
    "        self.centroids = X[np.random.choice(range(len(X)), self.k, replace=False)]\n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            # Assign each data point to the nearest centroid\n",
    "            cluster_assignments = []\n",
    "            for j in range(len(X)):\n",
    "                distances = np.linalg.norm(X[j] - self.centroids, axis=1)\n",
    "                cluster_assignments.append(np.argmin(distances))\n",
    "            \n",
    "            # Update centroids\n",
    "            for k in range(self.k):\n",
    "                cluster_data_points = X[np.where(np.array(cluster_assignments) == k)]\n",
    "                if len(cluster_data_points) > 0:\n",
    "                    self.centroids[k] = np.mean(cluster_data_points, axis=0)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if i > 0 and np.array_equal(self.centroids, previous_centroids):\n",
    "                break\n",
    "            \n",
    "            # Update previous centroids\n",
    "            previous_centroids = np.copy(self.centroids)\n",
    "        \n",
    "        # Store the final cluster assignments\n",
    "        self.cluster_assignments = cluster_assignments\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Assign each data point to the nearest centroid\n",
    "        cluster_assignments = []\n",
    "        for j in range(len(X)):\n",
    "            distances = np.linalg.norm(X[j] - self.centroids, axis=1)\n",
    "            cluster_assignments.append(np.argmin(distances))\n",
    "        \n",
    "        return cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5db6904-b2b5-45e4-a553-28f2c505b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[-5.30673591 -4.52413101]\n",
      " [ 5.04505704  4.53216316]]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.randn(5,2) + 5\n",
    "x2 = np.random.randn(5,2) - 5\n",
    "X = np.concatenate([x1,x2], axis=0)\n",
    "\n",
    "# Initialize the KMeans object with k=3\n",
    "kmeans = KMeans(k=2)\n",
    "\n",
    "# Fit the k-means model to the dataset\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster assignments for the input dataset\n",
    "cluster_assignments = kmeans.predict(X)\n",
    "\n",
    "# Print the cluster assignments\n",
    "print(cluster_assignments)\n",
    "\n",
    "# Print the learned centroids\n",
    "print(kmeans.centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f213669-7b8b-4b2f-a43c-6bfa4f0fbbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Plot the data points with different colors based on their cluster assignments\n",
    "colors = ['r', 'b']\n",
    "for i in range(kmeans.k):\n",
    "    plt.scatter(X[np.where(np.array(cluster_assignments) == i)][:,0], \n",
    "                X[np.where(np.array(cluster_assignments) == i)][:,1], \n",
    "                color=colors[i])\n",
    "\n",
    "# Plot the centroids as black circles\n",
    "plt.scatter(kmeans.centroids[:,0], kmeans.centroids[:,1], color='black', marker='o')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78cc6b22-cb44-4328-821e-0d973721b1b7",
   "metadata": {},
   "source": [
    "Optimization\n",
    "Here are some ways to optimize the k-means clustering algorithm:\n",
    "\n",
    "Random initialization of centroids: Instead of initializing the centroids using the first k data points, we can randomly initialize them to improve the convergence of the algorithm. This can be done by selecting k random data points from the input dataset as the initial centroids.\n",
    "\n",
    "Early stopping: We can stop the k-means algorithm if the cluster assignments and centroids do not change after a certain number of iterations. This helps to avoid unnecessary computation.\n",
    "\n",
    "Vectorization: We can use numpy arrays and vectorized operations to speed up the computation. This avoids the need for loops and makes the code more efficient.\n",
    "\n",
    "Here's an optimized version of the k-means clustering algorithm that implements these optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37ae647a-d071-4356-b167-48a33123838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k=3, max_iters=100, tol=1e-4):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self, X):\n",
    "        # Initialize centroids randomly\n",
    "        self.centroids = X[np.random.choice(X.shape[0], self.k, replace=False)]\n",
    "        \n",
    "        # Iterate until convergence or maximum number of iterations is reached\n",
    "        for i in range(self.max_iters):\n",
    "            # Assign each data point to the closest centroid\n",
    "            distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "            cluster_assignments = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update the centroids based on the new cluster assignments\n",
    "            new_centroids = np.array([np.mean(X[np.where(cluster_assignments == j)], axis=0) \n",
    "                                      for j in range(self.k)])\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(new_centroids - self.centroids) < self.tol:\n",
    "                break\n",
    "                \n",
    "            self.centroids = new_centroids\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Assign each data point to the closest centroid\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        cluster_assignments = np.argmin(distances, axis=1)\n",
    "        \n",
    "        return cluster_assignments"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f58f66c0-81e4-4681-9313-0f6ad46c3f9b",
   "metadata": {},
   "source": [
    "ðŸ“Œ Step-by-step:\n",
    "1ï¸âƒ£ Reshape X:\n",
    "X[:, np.newaxis] \n",
    "array([\n",
    "    [[1, 2]],     # shape (1, 2)\n",
    "    [[4, 5]]\n",
    "])               # shape becomes (2, 1, 2)\n",
    "\n",
    "\n",
    "2ï¸âƒ£ Broadcast Subtraction:\n",
    "X[:, np.newaxis] - self.centroids\n",
    "Shape: (2, 2, 2)\n",
    "\n",
    "For each data point, you subtract both centroids:\n",
    "# For point [1, 2]:\n",
    "[[1, 2] - [1, 0] = [0, 2],\n",
    " [1, 2] - [5, 5] = [-4, -3]]\n",
    "\n",
    "# For point [4, 5]:\n",
    "[[4, 5] - [1, 0] = [3, 5],\n",
    " [4, 5] - [5, 5] = [-1, 0]]\n",
    "So the full result is:\n",
    "array([\n",
    "    [[ 0,  2], [-4, -3]],\n",
    "    [[ 3,  5], [-1,  0]]\n",
    "])\n",
    "\n",
    "\n",
    "3ï¸âƒ£ Take the norm along axis=2:\n",
    "np.linalg.norm(..., axis=2)\n",
    "Compute Euclidean distance for each [i][j]:\n",
    "\n",
    "First row:\n",
    "\n",
    "âˆš(0Â² + 2Â²) = âˆš4 = 2\n",
    "\n",
    "âˆš((-4)Â² + (-3)Â²) = âˆš(16 + 9) = âˆš25 = 5\n",
    "\n",
    "Second row:\n",
    "\n",
    "âˆš(3Â² + 5Â²) = âˆš(9 + 25) = âˆš34 â‰ˆ 5.83\n",
    "\n",
    "âˆš((-1)Â² + 0Â²) = 1\n",
    "\n",
    "So the final result is:\n",
    "distances = np.array([\n",
    "    [2.0, 5.0],\n",
    "    [5.83, 1.0]\n",
    "])\n",
    "\n",
    "\n",
    "âœ… Interpretation:\n",
    "Row 0 (point [1, 2]) is closer to centroid 0 (distance = 2.0).\n",
    "\n",
    "Row 1 (point [4, 5]) is closer to centroid 1 (distance = 1.0).\n",
    "\n",
    "So if you do:\n",
    "np.argmin(distances, axis=1)\n",
    "You get:\n",
    "array([0, 1])\n",
    "That means:\n",
    "\n",
    "Point [1, 2] assigned to cluster 0\n",
    "\n",
    "Point [4, 5] assigned to cluster 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075e54c-677e-42fa-beca-d240b1f43ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
