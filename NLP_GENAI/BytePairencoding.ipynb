{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bpe_merges = []\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    def get_stats(self, corpus):\n",
    "        pairs = defaultdict(int)\n",
    "        for word in corpus:\n",
    "            for i in range(len(word)-1):\n",
    "                pair = (word[i], word[i+1])\n",
    "                pairs[pair] += 1\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, corpus, pair):\n",
    "        new_corpus = []\n",
    "        pattern = re.escape(' '.join(pair))\n",
    "        regex = re.compile(r'(?<!\\S)' + pattern + r'(?!\\S)')\n",
    "\n",
    "        for word in corpus:\n",
    "            joined = ' '.join(word)\n",
    "            replaced = regex.sub(''.join(pair), joined)\n",
    "            new_word = replaced.split()\n",
    "            new_corpus.append(new_word)\n",
    "        return new_corpus\n",
    "\n",
    "    def train(self, text):\n",
    "        corpus = [list(word) + ['</w>'] for word in text.split()]\n",
    "        corpus = [tuple(word) for word in corpus]\n",
    "\n",
    "        for _ in range(self.vocab_size):\n",
    "            pairs = self.get_stats(corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.bpe_merges.append(best)\n",
    "            corpus = self.merge_vocab(corpus, best)\n",
    "\n",
    "        # Create vocab and reverse map\n",
    "        tokens = set()\n",
    "        for word in corpus:\n",
    "            tokens.update(word)\n",
    "\n",
    "        self.token_to_id = {tok: i for i, tok in enumerate(tokens)}\n",
    "        self.id_to_token = {i: tok for tok, i in self.token_to_id.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "        for word in text.split():\n",
    "            word = list(word) + ['</w>']\n",
    "            for pair in self.bpe_merges:\n",
    "                i = 0\n",
    "                while i < len(word) - 1:\n",
    "                    if word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                        word[i:i+2] = [''.join(pair)]\n",
    "                    else:\n",
    "                        i += 1\n",
    "            tokens.extend(word)\n",
    "        return [self.token_to_id[token] for token in tokens if token in self.token_to_id]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = [self.id_to_token[i] for i in ids]\n",
    "        return ''.join([t.replace('</w>', ' ') for t in tokens]).strip()\n",
    "        \n",
    "        \n",
    " # Test\n",
    "bpe = BPETokenizer(vocab_size=50)\n",
    "bpe.train(\"low lowest lower lowest low\")\n",
    "\n",
    "encoded = bpe.encode(\"lowest low\")\n",
    "print(\"Encoded IDs:\", encoded) \n",
    "'''\n",
    "Encoded IDs: [1, 2]\n",
    "'''\n",
    "\n",
    "decoded = bpe.decode(encoded)\n",
    "print(\"Decoded Text:\", decoded)\n",
    "'''\n",
    "Decoded Text: lowest low\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
